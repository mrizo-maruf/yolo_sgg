import torch
import torch.cuda as cuda
import numpy as np
import os
import sys
from pathlib import Path

sys.path.append('/home/amrl/coda')
sys.path.append('/home/amrl/coda/yolo_sgg')

class ModelConfig:
    NAME = 'YOLOSGG'
    BACKBONE = 'PointNet2'
    FEATURE_DIM = 256
    NUM_CLASSES = 10
    NUM_RELATIONS = 20
    USE_IMAGE = True
    IMAGE_BACKBONE = 'ResNet50'
    FUSION_STRATEGY = 'early'

class DataConfig:
    NAME = 'CODA'
    ROOT_DIR = '/home/amrl/coda/data/CODA'
    NUM_POINTS = 16384
    POINT_CLOUD_RANGE = [-50, -50, -5, 50, 50, 3]
    VOXEL_SIZE = [0.1, 0.1, 0.2]
    CLASS_NAMES = [
        'car', 'truck', 'bus', 'motorcycle', 'bicycle',
        'pedestrian', 'traffic_light', 'stop_sign', 'building', 'vegetation'
    ]
    RELATION_NAMES = [
        'left_of', 'right_of', 'in_front_of', 'behind',
        'above', 'below', 'near', 'far_from',
        'same_as', 'different_from', 'inside', 'outside',
        'attached_to', 'part_of', 'supported_by', 'supporting',
        'occluding', 'occluded_by', 'moving_toward', 'moving_away'
    ]
    NUM_CLASSES = 10

class Config:
    MODEL = ModelConfig()
    DATA = DataConfig()

from models.yolo_sgg import YOLOSGG
from datasets.coda_dataset import CODADataset
from utils.transforms import get_transforms

class CODAEvaluator:
    def __init__(self, checkpoint_path, data_root, split='test'):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.cfg = Config()
        self.model = YOLOSGG(self.cfg.MODEL).to(self.device)
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['state_dict'])
        self.model.eval()
        
        class TransformConfig:
            def __init__(self, cfg):
                self.num_points = cfg.DATA.NUM_POINTS
                self.point_cloud_range = cfg.DATA.POINT_CLOUD_RANGE
                self.voxel_size = cfg.DATA.VOXEL_SIZE
                
        transform_cfg = TransformConfig(self.cfg)
        transforms = get_transforms(transform_cfg, is_train=False)
        self.dataset = CODADataset(root_dir=data_root, split=split, transform=transforms)
        self.metrics = {'detection': {}, 'relationship': {}, 'scene_graph': {}}

    def compute_3d_iou(self, box1, box2):
        corners1 = self.box_to_corners(box1)
        corners2 = self.box_to_corners(box2)
        vol_inter = self.compute_volume_intersection(corners1, corners2)
        vol1 = box1[:, 3] * box1[:, 4] * box1[:, 5]
        vol2 = box2[:, 3] * box2[:, 4] * box2[:, 5]
        vol_union = vol1.unsqueeze(1) + vol2.unsqueeze(0) - vol_inter
        iou = vol_inter / (vol_union + 1e-8)
        return iou

    def box_to_corners(self, boxes):
        device = boxes.device
        batch_size = boxes.shape[0]
        center = boxes[:, :3]
        dims = boxes[:, 3:6]
        angles = boxes[:, 6]
        corners = torch.zeros((batch_size, 8, 3), device=device)
        std_corners = torch.tensor([
            [-0.5, -0.5, -0.5], [-0.5, -0.5, 0.5], [-0.5, 0.5, -0.5], [-0.5, 0.5, 0.5],
            [0.5, -0.5, -0.5], [0.5, -0.5, 0.5], [0.5, 0.5, -0.5], [0.5, 0.5, 0.5]
        ], device=device)
        cos_a = torch.cos(angles).unsqueeze(1).unsqueeze(2)
        sin_a = torch.sin(angles).unsqueeze(1).unsqueeze(2)
        rot_mat = torch.cat([
            torch.cat([cos_a, -sin_a, torch.zeros_like(cos_a)], dim=2),
            torch.cat([sin_a, cos_a, torch.zeros_like(cos_a)], dim=2),
            torch.cat([torch.zeros_like(cos_a), torch.zeros_like(cos_a), torch.ones_like(cos_a)], dim=2)
        ], dim=1)
        for i in range(batch_size):
            corners[i] = torch.matmul(std_corners * dims[i], rot_mat[i].T) + center[i]
        return corners

    def compute_volume_intersection(self, corners1, corners2):
        N, M = corners1.shape[0], corners2.shape[0]
        vol_inter = torch.zeros((N, M), device=corners1.device)
        for i in range(N):
            for j in range(M):
                min1 = corners1[i].min(dim=0)[0]
                max1 = corners1[i].max(dim=0)[0]
                min2 = corners2[j].min(dim=0)[0]
                max2 = corners2[j].max(dim=0)[0]
                inter_min = torch.max(min1, min2)
                inter_max = torch.min(max1, max2)
                if (inter_max > inter_min).all():
                    inter_dims = inter_max - inter_min
                    vol_inter[i, j] = inter_dims.prod()
        return vol_inter

    def compute_detection_metrics(self, predictions, ground_truth, iou_thresholds=[0.25, 0.5, 0.75]):
        all_aps = []
        metrics = {}
        for iou_thresh in iou_thresholds:
            aps_per_class = []
            for class_id in range(self.cfg.DATA.NUM_CLASSES):
                pred_boxes = []
                pred_scores = []
                gt_boxes = []
                for pred, gt in zip(predictions, ground_truth):
                    pred_mask = pred['class_ids'] == class_id
                    gt_mask = gt['class_ids'] == class_id
                    if pred_mask.any():
                        pred_boxes.append(pred['boxes'][pred_mask])
                        pred_scores.append(pred['scores'][pred_mask])
                    if gt_mask.any():
                        gt_boxes.append(gt['boxes'][gt_mask])
                if not pred_boxes or not gt_boxes:
                    aps_per_class.append(0.0)
                    continue
                ap = self.compute_average_precision(pred_boxes, pred_scores, gt_boxes, iou_thresh)
                aps_per_class.append(ap)
            map_score = np.mean(aps_per_class) if aps_per_class else 0.0
            all_aps.append(map_score)
            metrics[f'AP@{iou_thresh}'] = map_score
        metrics['mAP'] = np.mean(all_aps)
        return metrics

    def compute_average_precision(self, pred_boxes_list, pred_scores_list, gt_boxes_list, iou_threshold):
        all_pred_boxes = []
        all_pred_scores = []
        all_pred_img_ids = []
        for img_id, (pred_boxes, pred_scores) in enumerate(zip(pred_boxes_list, pred_scores_list)):
            if len(pred_boxes) > 0:
                all_pred_boxes.append(pred_boxes)
                all_pred_scores.append(pred_scores)
                all_pred_img_ids.extend([img_id] * len(pred_boxes))
        if not all_pred_boxes:
            return 0.0
        all_pred_boxes = torch.cat(all_pred_boxes, dim=0)
        all_pred_scores = torch.cat(all_pred_scores, dim=0)
        sorted_indices = torch.argsort(all_pred_scores, descending=True)
        all_pred_boxes = all_pred_boxes[sorted_indices]
        all_pred_img_ids = [all_pred_img_ids[i] for i in sorted_indices]
        tp = np.zeros(len(all_pred_boxes))
        fp = np.zeros(len(all_pred_boxes))
        gt_matched = []
        for gt_boxes in gt_boxes_list:
            gt_matched.append(np.zeros(len(gt_boxes)))
        for i, (pred_box, img_id) in enumerate(zip(all_pred_boxes, all_pred_img_ids)):
            gt_boxes = gt_boxes_list[img_id]
            if len(gt_boxes) == 0:
                fp[i] = 1
                continue
            ious = self.compute_3d_iou(pred_box.unsqueeze(0), gt_boxes)
            max_iou = ious.max().item()
            max_idx = ious.argmax().item()
            if max_iou >= iou_threshold and not gt_matched[img_id][max_idx]:
                tp[i] = 1
                gt_matched[img_id][max_idx] = 1
            else:
                fp[i] = 1
        tp_cumsum = np.cumsum(tp)
        fp_cumsum = np.cumsum(fp)
        recalls = tp_cumsum / max(len([g for g in gt_boxes_list if len(g) > 0]), 1)
        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)
        ap = 0.0
        for t in np.arange(0, 1.01, 0.01):
            mask = recalls >= t
            if mask.any():
                ap += np.max(precisions[mask]) / 101.0
        return ap

    def compute_relationship_metrics(self, predictions, ground_truth, k_values=[20, 50, 100]):
        metrics = {}
        all_recalls = {k: [] for k in k_values}
        for pred, gt in zip(predictions, ground_truth):
            if 'relationships' not in pred or 'relationships' not in gt:
                continue
            pred_triplets = pred['relationships']
            gt_triplets = gt['relationships']
            if len(gt_triplets) == 0:
                continue
            for k in k_values:
                if len(pred_triplets) >= k:
                    matches = 0
                    for gt_triplet in gt_triplets:
                        for pred_triplet in pred_triplets[:k]:
                            if self.triplets_match(gt_triplet, pred_triplet):
                                matches += 1
                                break
                    recall = matches / len(gt_triplets)
                    all_recalls[k].append(recall)
        for k in k_values:
            if all_recalls[k]:
                metrics[f'R@{k}'] = np.mean(all_recalls[k])
            else:
                metrics[f'R@{k}'] = 0.0
        return metrics

    def triplets_match(self, triplet1, triplet2):
        return (triplet1[0] == triplet2[0] and triplet1[1] == triplet2[1] and triplet1[2] == triplet2[2])

    def compute_scene_graph_metrics(self, predictions, ground_truth):
        metrics = {}
        similarities = []
        for pred, gt in zip(predictions, ground_truth):
            if 'scene_graph' in pred and 'scene_graph' in gt:
                similarity = self.compute_graph_similarity(pred['scene_graph'], gt['scene_graph'])
                similarities.append(similarity)
        if similarities:
            metrics['Graph_Similarity'] = np.mean(similarities)
            metrics['Graph_Similarity_std'] = np.std(similarities)
        return metrics

    def compute_graph_similarity(self, graph1, graph2):
        common_nodes = 0
        if 'nodes' in graph1 and 'nodes' in graph2:
            nodes1 = set(graph1['nodes'])
            nodes2 = set(graph2['nodes'])
            common_nodes = len(nodes1.intersection(nodes2))
            total_nodes = len(nodes1.union(nodes2))
        common_edges = 0
        if 'edges' in graph1 and 'edges' in graph2:
            edges1 = set([tuple(e) for e in graph1['edges']])
            edges2 = set([tuple(e) for e in graph2['edges']])
            common_edges = len(edges1.intersection(edges2))
            total_edges = len(edges1.union(edges2))
        node_sim = common_nodes / total_nodes if total_nodes > 0 else 0
        edge_sim = common_edges / total_edges if total_edges > 0 else 0
        return 0.5 * node_sim + 0.5 * edge_sim

    def evaluate(self, num_samples=None):
        predictions = []
        ground_truth = []
        total_samples = len(self.dataset)
        if num_samples is not None:
            total_samples = min(num_samples, total_samples)
        with torch.no_grad():
            for idx in range(total_samples):
                data = self.dataset[idx]
                point_cloud = data['point_cloud'].to(self.device).unsqueeze(0)
                image = data['image'].to(self.device).unsqueeze(0) if 'image' in data else None
                outputs = self.model(point_cloud, image)
                pred_dict = self.process_outputs(outputs, data)
                predictions.append(pred_dict)
                gt_dict = {
                    'boxes': data['boxes_3d'],
                    'class_ids': data['class_ids'],
                    'relationships': data.get('relationships', []),
                    'scene_graph': data.get('scene_graph', {})
                }
                ground_truth.append(gt_dict)
        self.metrics['detection'] = self.compute_detection_metrics(predictions, ground_truth)
        self.metrics['relationship'] = self.compute_relationship_metrics(predictions, ground_truth)
        self.metrics['scene_graph'] = self.compute_scene_graph_metrics(predictions, ground_truth)
        self.metrics['overall'] = self.compute_overall_score()
        return self.metrics

    def process_outputs(self, outputs, data):
        pred_dict = {}
        if 'boxes_3d' in outputs:
            pred_dict['boxes'] = outputs['boxes_3d'].cpu()
            pred_dict['scores'] = outputs['scores'].cpu()
            pred_dict['class_ids'] = outputs['class_ids'].cpu()
        if 'relationships' in outputs:
            pred_dict['relationships'] = outputs['relationships'].cpu().numpy().tolist()
        if 'scene_graph' in outputs:
            pred_dict['scene_graph'] = outputs['scene_graph']
        return pred_dict

    def compute_overall_score(self):
        weights = {'detection': 0.4, 'relationship': 0.4, 'scene_graph': 0.2}
        score = 0.0
        if 'mAP' in self.metrics['detection']:
            score += weights['detection'] * self.metrics['detection']['mAP']
        if self.metrics['relationship']:
            r_values = [v for k, v in self.metrics['relationship'].items() if k.startswith('R@')]
            if r_values:
                score += weights['relationship'] * np.mean(r_values)
        if 'Graph_Similarity' in self.metrics['scene_graph']:
            score += weights['scene_graph'] * self.metrics['scene_graph']['Graph_Similarity']
        return score

    def print_results(self):
        print("Detection Metrics:")
        for key, value in self.metrics['detection'].items():
            print(f"{key}: {value:.4f}")
        print("\nRelationship Metrics:")
        for key, value in self.metrics['relationship'].items():
            print(f"{key}: {value:.4f}")
        print("\nScene Graph Metrics:")
        for key, value in self.metrics['scene_graph'].items():
            print(f"{key}: {value:.4f}")
        print(f"\nOverall Score: {self.metrics['overall']:.4f}")

def main():
    CHECKPOINT_PATH = "/home/amrl/coda/checkpoints/yolo_sgg_coda.pth"
    DATA_ROOT = "/home/amrl/coda/data/CODA"
    
    if not torch.cuda.is_available():
        print("CUDA not available")
        return
    
    torch.manual_seed(42)
    np.random.seed(42)
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
    
    evaluator = CODAEvaluator(checkpoint_path=CHECKPOINT_PATH, data_root=DATA_ROOT, split='val')
    metrics = evaluator.evaluate(num_samples=100)
    evaluator.print_results()

if __name__ == "__main__":
    main()
